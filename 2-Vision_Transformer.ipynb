{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Vision Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En este Jupyter Notebook, vamos a ajustar un Vision Transformer pre-entrenado (de [🤗 Transformers](https://github.com/huggingface/transformers)) para la clasificación de imágenes. Entrenaremos el modelo usando [PyTorch Lightning ⚡](https://github.com/PyTorchLightning/pytorch-lightning).\n",
    "\n",
    "HuggingFace 🤗 es una comunidad y biblioteca líder de software de código abierto que ha ganado una atención significativa en los últimos años por sus contribuciones a la democratización de la inteligencia artificial. La biblioteca proporciona modelos pre-entrenados, conjuntos de datos y una suite de herramientas que hacen que sea más fácil para los desarrolladores construir y desplegar aplicaciones de inteligencia artificial. Una de las contribuciones más significativas de HuggingFace es el desarrollo de la biblioteca Transformers, que proporciona una interfaz fácil de usar para trabajar con modelos basados en Transformer, como BERT y GPT.\n",
    "\n",
    "PyTorch Lightning es una biblioteca de Python de código abierto que proporciona una interfaz de alto nivel para PyTorch. Este framework liviano y de alto rendimiento organiza el código de PyTorch para desacoplar la investigación de la ingeniería, haciendo que los experimentos de Deep Learning sean más fáciles de leer y reproducir.\n",
    "\n",
    "**Fuente:** Rogge, N. (2021) [Fine-tuning the Vision Transformer on CIFAR-10 with PyTorch Lightning - GitHub](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb).\n",
    "\n",
    "![vit.png](./docs/Vision_Transformer.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué son los Transformers?\n",
    "\n",
    "La arquitectura Transformer, que fue presentada en el artículo \"Attention is All You Need\" en 2017, ha revolucionado el mundo del Deep Learning, especialmente en el campo del Procesamiento del Lenguaje Natural. Como un gran modelo de lenguaje basado en la arquitectura GPT-3.5, ChatGPT es la aplicación basada en la arquitectura Transformer más popular del momento. Además de ChatGPT, muchas otras aplicaciones reconocidas, como BERT de Google, la serie GPT de OpenAI y RoBERTa de Facebook, se basan en la arquitectura Transformer para lograr resultados de vanguardia en tareas de NLP. Además, la arquitectura Transformer también ha tenido un gran éxito en el campo de la Visión por Computador, como lo demuestra el éxito de modelos como ViT y DeiT en ImageNet y otros benchmarks de reconocimiento visual.\n",
    "\n",
    "La principal innovación de la arquitectura Transformer es la combinación del uso de representaciones basadas en atención y un estilo de procesamiento similar al de una red neuronal convolucional (CNN). A diferencia de las redes neuronales convolucionales tradicionales (CNN) que se basan en capas convolucionales para extraer características de las imágenes, los Transformers utilizan mecanismos de atención (auto-atención, atención multi-cabezal) para enfocarse selectivamente en diferentes partes de una secuencia de entrada.\n",
    "\n",
    "La principal ventaja de los Transformers sobre las CNN tradicionales es que pueden capturar de manera más efectiva las dependencias a largo plazo en los datos. Esto es especialmente útil en tareas de visión por computadora donde una imagen puede contener objetos que están dispersos por toda la imagen, y donde las relaciones entre objetos pueden ser más importantes que los propios objetos. Al atender a diferentes partes de la imagen de entrada, los Transformers pueden aprender eficazmente a extraer estas relaciones y mejorar el rendimiento en tareas como la detección y segmentación de objetos.\n",
    "\n",
    "\n",
    "**Fuentes:**\n",
    "\n",
    "+ Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention is all you need.](https://arxiv.org/abs/1706.03762) - arXiv preprint arXiv:1706.03762. \n",
    "+ Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) - arXiv preprint arXiv:2010.11929.\n",
    "+ Google Research. (2021). [Vision Transformer and MLP-Mixer Architectures  - GitHub](https://github.com/google-research/vision_transformer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeros pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install -q transformers datasets pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venvs\\no-estruc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from src.vit_fine_tune import ViTLightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory where to save the models created\n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activando CUDA para el procesamiento con GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las GPUs (Graphic Processing Units o Unidades de Procesamiento Gráfico) son procesadores especializados diseñados para manejar los cálculos complejos involucrados en la representación de gráficos e imágenes. Sin embargo, debido a sus capacidades de procesamiento paralelo, también son útiles para una amplia gama de otras aplicaciones, incluyendo el Aprendizaje Automático. A diferencia de las CPU tradicionales, las GPUs pueden manejar muchas tareas más pequeñas simultáneamente, lo que las hace ideales para aplicaciones computacionalmente intensivas.\n",
    "\n",
    "CUDA es una plataforma de cómputo paralelo y un modelo de programación desarrollado por NVIDIA, diseñado para aprovechar el poder de las GPUs para tareas de cómputo de propósito general. CUDA permite a los desarrolladores escribir programas que se ejecutan en la GPU, aprovechando sus capacidades de procesamiento paralelo para acelerar significativamente el rendimiento.\n",
    "\n",
    "Para acelerar significativamente el entrenamiento del modelo, utilizaremos la aceleración GPU. Primero comprobaremos si CUDA está disponible en nuestro sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.8\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "  \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "        \n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecha esta comprobación, realizamos el entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard es una herramienta de visualización basada en la web proporcionada por TensorFlow para visualizar y analizar varios aspectos de los experimentos de aprendizaje automático.\n",
    "\n",
    "El comando %load_ext tensorboard carga la extensión de TensorBoard en Jupyter Notebook. El comando %tensorboard --logdir lightning_logs/ inicia TensorBoard y especifica el directorio donde se almacenan los registros, en este caso ./lightning_logs/. TensorBoard lee los eventos y las métricas registradas durante el proceso de entrenamiento y proporciona visualizaciones para analizar el rendimiento del modelo, incluyendo curvas de pérdida y precisión, histogramas de pesos y sesgos, y más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fb76cef38166f85b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fb76cef38166f85b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\venvs\\no-estruc\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: d:\\Estudios\\Masters\\MBD_ICAI\\Cuatri_2\\ML\\Code\\Practica-DL\\ml2-deep-learning\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "d:\\venvs\\no-estruc\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type                      | Params\n",
      "---------------------------------------------------\n",
      "0 | vit  | ViTForImageClassification | 85.8 M\n",
      "---------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.241   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 187/187 [04:23<00:00,  1.41s/it, v_num=0]        \n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "model = ViTLightningModule()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    strict=False,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        early_stop_callback\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación del modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funciones para realizar comprobaciones sobre el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_pytorch_predictions_from_dataloader(model, dataloader):\n",
    "    \"\"\"\n",
    "    Get predictions from a Pytorch model on a given dataloader.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    model: PyTorch model\n",
    "        The model to use for predictions.\n",
    "    dataloader: PyTorch dataloader\n",
    "        The dataloader to use for predictions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    all_predictions: list\n",
    "        List of predictions.\n",
    "    all_targets: list\n",
    "        List of targets.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Set model to evaluation mode and freeze it\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "\n",
    "    # Lists to store predictions and targets\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Use a progress bar to show the progress of the predictions\n",
    "    for batch in tqdm(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device) # Move inputs to the same device as the model\n",
    "        predictions = model(images)\n",
    "        # Convert ImageClassifierOutput to tensor\n",
    "        predictions = predictions.logits\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    return all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python base libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "DATA_DIR = './src/dataset'\n",
    "TRAIN_DIR = DATA_DIR + '/training'\n",
    "VAL_DIR = DATA_DIR + '/validation'\n",
    "\n",
    "# Data Science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning and Deep Learning libraries\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    CenterCrop, \n",
    "    Compose, \n",
    "    Normalize, \n",
    "    Resize, \n",
    "    ToTensor\n",
    ")\n",
    "\n",
    "def get_vit_metrics(model, train=False):\n",
    "    \"\"\"\n",
    "    Gets the metrics for the ViT model.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    model: PyTorch model\n",
    "        The ViT model to use for predictions.\n",
    "    train: bool, optional (default=False)\n",
    "        Whether to get the metrics for the training set.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The classification report for the ViT model is printed\n",
    "        to the console for both the validation and test sets.\n",
    "    \"\"\"\n",
    "    # Get the image processor and its parameters\n",
    "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    img_size = processor.size\n",
    "    img_mean = processor.image_mean\n",
    "    img_std = processor.image_std\n",
    "\n",
    "    transform = Compose([\n",
    "        Resize(img_size['height']),\n",
    "        CenterCrop(img_size['height']),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=img_mean, std=img_std)\n",
    "    ])\n",
    "\n",
    "    # Get the classes from the model\n",
    "    classes = model.id2label.values()\n",
    "\n",
    "    # Get the dataloaders for the validation set\n",
    "    val_dataset = ImageFolder(VAL_DIR, transform=transform)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    if train:\n",
    "        transform_train = Compose([\n",
    "            RandomResizedCrop(img_size['height']),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=img_mean, std=img_std)\n",
    "        ])\n",
    "        # Get the dataloader for the training set\n",
    "        train_dataset = ImageFolder(TRAIN_DIR, transform=transform_train)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "        # Get the classification report for the training set\n",
    "        predictions, targets = get_pytorch_predictions_from_dataloader(model, train_dataloader)\n",
    "        print('Training set classification report:')\n",
    "        print(classification_report(targets, predictions.argmax(dim=1), target_names=classes))\n",
    "\n",
    "    # Get the classification report for the validation set\n",
    "    predictions, targets = get_pytorch_predictions_from_dataloader(model, val_dataloader)\n",
    "    print('Validation set classification report:')\n",
    "    print(classification_report(targets, predictions.argmax(dim=1), target_names=classes))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIGHTNING_LOGS_DIR = './lightning_logs'\n",
    "def load_latest_checkpoint(model_class, logs_dir=LIGHTNING_LOGS_DIR):\n",
    "    \"\"\"\n",
    "    Loads the latest checkpoint from the lightning_logs directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    model_class: PyTorch Lightning model class\n",
    "        The model class to use for loading the checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model: PyTorch Lightning model\n",
    "        The model loaded from the latest checkpoint.\n",
    "    \"\"\"\n",
    "    version_dirs = glob.glob(os.path.join(logs_dir, 'version_*'))\n",
    "    latest_version_dir = max(version_dirs, key=os.path.getmtime)\n",
    "    ckpt_files = glob.glob(os.path.join(latest_version_dir, 'checkpoints', '*.ckpt'))\n",
    "    latest_ckpt_file = max(ckpt_files, key=os.path.getmtime)\n",
    "    \n",
    "    # Load the checkpoint into a new instance of the model class\n",
    "    model = model_class.load_from_checkpoint(latest_ckpt_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [01:12<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Bedroom       0.86      0.96      0.91       116\n",
      "        Coast       0.91      0.98      0.94       260\n",
      "       Forest       0.98      0.96      0.97       228\n",
      "      Highway       1.00      0.89      0.94       160\n",
      "   Industrial       0.97      0.91      0.94       211\n",
      "  Inside city       0.95      0.93      0.94       208\n",
      "      Kitchen       0.97      0.99      0.98       110\n",
      "  Living room       0.98      0.88      0.93       189\n",
      "     Mountain       0.90      0.99      0.94       274\n",
      "       Office       0.99      0.99      0.99       115\n",
      " Open country       0.96      0.85      0.90       310\n",
      "        Store       0.95      1.00      0.97       215\n",
      "       Street       0.89      0.98      0.94       192\n",
      "       Suburb       0.99      0.98      0.98       141\n",
      "Tall building       0.96      0.96      0.96       256\n",
      "\n",
      "     accuracy                           0.95      2985\n",
      "    macro avg       0.95      0.95      0.95      2985\n",
      " weighted avg       0.95      0.95      0.95      2985\n",
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:36<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Bedroom       0.86      0.96      0.91       100\n",
      "        Coast       0.89      0.97      0.93       100\n",
      "       Forest       1.00      0.89      0.94       100\n",
      "      Highway       0.98      0.92      0.95       100\n",
      "   Industrial       0.96      0.81      0.88       100\n",
      "  Inside city       0.86      0.86      0.86       100\n",
      "      Kitchen       0.99      0.86      0.92       100\n",
      "  Living room       0.88      0.87      0.87       100\n",
      "     Mountain       0.92      1.00      0.96       100\n",
      "       Office       0.98      0.99      0.99       100\n",
      " Open country       0.88      0.81      0.84       100\n",
      "        Store       0.88      0.96      0.92       100\n",
      "       Street       0.88      0.98      0.93       100\n",
      "       Suburb       1.00      0.96      0.98       100\n",
      "Tall building       0.89      0.97      0.93       100\n",
      "\n",
      "     accuracy                           0.92      1500\n",
      "    macro avg       0.92      0.92      0.92      1500\n",
      " weighted avg       0.92      0.92      0.92      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model from the latest checkpoint\n",
    "best_model = load_latest_checkpoint(ViTLightningModule)\n",
    "# Get best model metrics\n",
    "get_vit_metrics(best_model, train=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del conjunto de entrenamiento, la matriz de confusión indica que el modelo logró una precisión de alrededor del 95% en la clasificación de las imágenes, lo que significa que la mayoría de las imágenes fueron clasificadas correctamente. La mayoría de las clases tienen una precisión y recall bastante altos, con pocos casos de falsos positivos o negativos. Sin embargo, la clase \"Inside city\" tuvo una precisión un poco más baja en comparación con las otras clases.\n",
    "\n",
    "En el conjunto de validación, el modelo obtuvo una precisión del 92%, lo que significa que las imágenes se clasificaron correctamente en la mayoría de los casos. En general, la mayoría de las clases tuvieron una precisión y recall similares a los del conjunto de entrenamiento, pero algunas clases, como \"Inside city\" y \"Open country\", tuvieron una precisión ligeramente más baja. Se puede concluir que el modelo tuvo un excelente rendimiento en la clasificación de las imágenes, aunque la precisión y recall varíen según la clase y el conjunto de datos.\n",
    "\n",
    "En definitiva, el modelo fine-tuneado de Vision Transformer obtiene un rendimiento excelente, el cual está al nivel de los mejores modelos de CNN. Además, no se observa una diferencia significativa en términos de performance entre los conjuntos de entrenamiento y validación, lo que sugiere una gran capacidad de generalización a nuevos datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo final en `vit_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the models directory\n",
    "torch.save(best_model.state_dict(), os.path.join(models_dir, \"vit_model.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, pese a no haber realizado ningún tuneo el Vision Transformer iguala al mejor modelo de CNN del Hackaton y muestra también una gran capacidad de generalización, con un rendimiento de 0.95 de accuracy en training y 0.92 en validation. Podemos deducir que este buen rendimiento del modelo Vision Transformer se debe a su capacidad para capturar las dependencias y las interacciones globales entre las características. Mientras que los modelos CNN tradicionales se basan en operaciones convolucionales y de agrupación para extraer características locales y aplanarlas en un vector, los transformers utilizan mecanismos de autoatención que permiten interacciones globales entre todas las características. Esto permite que los transformers modelen relaciones complejas entre las características e identifiquen dependencias a larga distancia, lo que los hace particularmente efectivos para tareas como la clasificación de imágenes.\n",
    "\n",
    "Además, la arquitectura jerárquica del Vision Transformer también puede contribuir a su éxito en la tarea de clasificación de estilo artístico. La arquitectura le permite procesar imágenes a múltiples niveles de granularidad, desde características locales hasta la imagen completa. Esto permite que el modelo aprenda representaciones que son más adecuadas para tareas como la clasificación de imágenes, y podría explicar su fuerte rendimiento en este proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "no-estruc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2818b84fe0d8ee9ed89d361455090ef436eee20ee147624d1f156870c67bd555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
