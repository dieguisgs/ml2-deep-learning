{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Vision Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducci贸n\n",
    "\n",
    "En este Jupyter Notebook, vamos a ajustar un Vision Transformer pre-entrenado (de [ Transformers](https://github.com/huggingface/transformers)) para la clasificaci贸n de im谩genes. Entrenaremos el modelo usando [PyTorch Lightning ](https://github.com/PyTorchLightning/pytorch-lightning).\n",
    "\n",
    "HuggingFace  es una comunidad y biblioteca l铆der de software de c贸digo abierto que ha ganado una atenci贸n significativa en los 煤ltimos a帽os por sus contribuciones a la democratizaci贸n de la inteligencia artificial. La biblioteca proporciona modelos pre-entrenados, conjuntos de datos y una suite de herramientas que hacen que sea m谩s f谩cil para los desarrolladores construir y desplegar aplicaciones de inteligencia artificial. Una de las contribuciones m谩s significativas de HuggingFace es el desarrollo de la biblioteca Transformers, que proporciona una interfaz f谩cil de usar para trabajar con modelos basados en Transformer, como BERT y GPT.\n",
    "\n",
    "PyTorch Lightning es una biblioteca de Python de c贸digo abierto que proporciona una interfaz de alto nivel para PyTorch. Este framework liviano y de alto rendimiento organiza el c贸digo de PyTorch para desacoplar la investigaci贸n de la ingenier铆a, haciendo que los experimentos de Deep Learning sean m谩s f谩ciles de leer y reproducir.\n",
    "\n",
    "**Fuente:** Rogge, N. (2021) [Fine-tuning the Vision Transformer on CIFAR-10 with PyTorch Lightning - GitHub](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb).\n",
    "\n",
    "![vit.png](./docs/Vision_Transformer.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 驴Qu茅 son los Transformers?\n",
    "\n",
    "La arquitectura Transformer, que fue presentada en el art铆culo \"Attention is All You Need\" en 2017, ha revolucionado el mundo del Deep Learning, especialmente en el campo del Procesamiento del Lenguaje Natural. Como un gran modelo de lenguaje basado en la arquitectura GPT-3.5, ChatGPT es la aplicaci贸n basada en la arquitectura Transformer m谩s popular del momento. Adem谩s de ChatGPT, muchas otras aplicaciones reconocidas, como BERT de Google, la serie GPT de OpenAI y RoBERTa de Facebook, se basan en la arquitectura Transformer para lograr resultados de vanguardia en tareas de NLP. Adem谩s, la arquitectura Transformer tambi茅n ha tenido un gran 茅xito en el campo de la Visi贸n por Computador, como lo demuestra el 茅xito de modelos como ViT y DeiT en ImageNet y otros benchmarks de reconocimiento visual.\n",
    "\n",
    "La principal innovaci贸n de la arquitectura Transformer es la combinaci贸n del uso de representaciones basadas en atenci贸n y un estilo de procesamiento similar al de una red neuronal convolucional (CNN). A diferencia de las redes neuronales convolucionales tradicionales (CNN) que se basan en capas convolucionales para extraer caracter铆sticas de las im谩genes, los Transformers utilizan mecanismos de atenci贸n (auto-atenci贸n, atenci贸n multi-cabezal) para enfocarse selectivamente en diferentes partes de una secuencia de entrada.\n",
    "\n",
    "La principal ventaja de los Transformers sobre las CNN tradicionales es que pueden capturar de manera m谩s efectiva las dependencias a largo plazo en los datos. Esto es especialmente 煤til en tareas de visi贸n por computadora donde una imagen puede contener objetos que est谩n dispersos por toda la imagen, y donde las relaciones entre objetos pueden ser m谩s importantes que los propios objetos. Al atender a diferentes partes de la imagen de entrada, los Transformers pueden aprender eficazmente a extraer estas relaciones y mejorar el rendimiento en tareas como la detecci贸n y segmentaci贸n de objetos.\n",
    "\n",
    "\n",
    "**Fuentes:**\n",
    "\n",
    "+ Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention is all you need.](https://arxiv.org/abs/1706.03762) - arXiv preprint arXiv:1706.03762. \n",
    "+ Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) - arXiv preprint arXiv:2010.11929.\n",
    "+ Google Research. (2021). [Vision Transformer and MLP-Mixer Architectures  - GitHub](https://github.com/google-research/vision_transformer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeros pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install -q transformers datasets pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venvs\\no-estruc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from src.vit_fine_tune import ViTLightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory where to save the models created\n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activando CUDA para el procesamiento con GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las GPUs (Graphic Processing Units o Unidades de Procesamiento Gr谩fico) son procesadores especializados dise帽ados para manejar los c谩lculos complejos involucrados en la representaci贸n de gr谩ficos e im谩genes. Sin embargo, debido a sus capacidades de procesamiento paralelo, tambi茅n son 煤tiles para una amplia gama de otras aplicaciones, incluyendo el Aprendizaje Autom谩tico. A diferencia de las CPU tradicionales, las GPUs pueden manejar muchas tareas m谩s peque帽as simult谩neamente, lo que las hace ideales para aplicaciones computacionalmente intensivas.\n",
    "\n",
    "CUDA es una plataforma de c贸mputo paralelo y un modelo de programaci贸n desarrollado por NVIDIA, dise帽ado para aprovechar el poder de las GPUs para tareas de c贸mputo de prop贸sito general. CUDA permite a los desarrolladores escribir programas que se ejecutan en la GPU, aprovechando sus capacidades de procesamiento paralelo para acelerar significativamente el rendimiento.\n",
    "\n",
    "Para acelerar significativamente el entrenamiento del modelo, utilizaremos la aceleraci贸n GPU. Primero comprobaremos si CUDA est谩 disponible en nuestro sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.8\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "  \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "        \n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecha esta comprobaci贸n, realizamos el entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard es una herramienta de visualizaci贸n basada en la web proporcionada por TensorFlow para visualizar y analizar varios aspectos de los experimentos de aprendizaje autom谩tico.\n",
    "\n",
    "El comando %load_ext tensorboard carga la extensi贸n de TensorBoard en Jupyter Notebook. El comando %tensorboard --logdir lightning_logs/ inicia TensorBoard y especifica el directorio donde se almacenan los registros, en este caso ./lightning_logs/. TensorBoard lee los eventos y las m茅tricas registradas durante el proceso de entrenamiento y proporciona visualizaciones para analizar el rendimiento del modelo, incluyendo curvas de p茅rdida y precisi贸n, histogramas de pesos y sesgos, y m谩s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fb76cef38166f85b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fb76cef38166f85b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\venvs\\no-estruc\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: d:\\Estudios\\Masters\\MBD_ICAI\\Cuatri_2\\ML\\Code\\Practica-DL\\ml2-deep-learning\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "d:\\venvs\\no-estruc\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type                      | Params\n",
      "---------------------------------------------------\n",
      "0 | vit  | ViTForImageClassification | 85.8 M\n",
      "---------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.241   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|| 187/187 [04:23<00:00,  1.41s/it, v_num=0]        \n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "model = ViTLightningModule()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    strict=False,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        early_stop_callback\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaci贸n del modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funciones para realizar comprobaciones sobre el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_pytorch_predictions_from_dataloader(model, dataloader):\n",
    "    \"\"\"\n",
    "    Get predictions from a Pytorch model on a given dataloader.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    model: PyTorch model\n",
    "        The model to use for predictions.\n",
    "    dataloader: PyTorch dataloader\n",
    "        The dataloader to use for predictions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    all_predictions: list\n",
    "        List of predictions.\n",
    "    all_targets: list\n",
    "        List of targets.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Set model to evaluation mode and freeze it\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "\n",
    "    # Lists to store predictions and targets\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Use a progress bar to show the progress of the predictions\n",
    "    for batch in tqdm(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device) # Move inputs to the same device as the model\n",
    "        predictions = model(images)\n",
    "        # Convert ImageClassifierOutput to tensor\n",
    "        predictions = predictions.logits\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    return all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python base libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "DATA_DIR = './src/dataset'\n",
    "TRAIN_DIR = DATA_DIR + '/training'\n",
    "VAL_DIR = DATA_DIR + '/validation'\n",
    "\n",
    "# Data Science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning and Deep Learning libraries\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    CenterCrop, \n",
    "    Compose, \n",
    "    Normalize, \n",
    "    Resize, \n",
    "    ToTensor\n",
    ")\n",
    "\n",
    "def get_vit_metrics(model, train=False):\n",
    "    \"\"\"\n",
    "    Gets the metrics for the ViT model.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    model: PyTorch model\n",
    "        The ViT model to use for predictions.\n",
    "    train: bool, optional (default=False)\n",
    "        Whether to get the metrics for the training set.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The classification report for the ViT model is printed\n",
    "        to the console for both the validation and test sets.\n",
    "    \"\"\"\n",
    "    # Get the image processor and its parameters\n",
    "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    img_size = processor.size\n",
    "    img_mean = processor.image_mean\n",
    "    img_std = processor.image_std\n",
    "\n",
    "    transform = Compose([\n",
    "        Resize(img_size['height']),\n",
    "        CenterCrop(img_size['height']),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=img_mean, std=img_std)\n",
    "    ])\n",
    "\n",
    "    # Get the classes from the model\n",
    "    classes = model.id2label.values()\n",
    "\n",
    "    # Get the dataloaders for the validation set\n",
    "    val_dataset = ImageFolder(VAL_DIR, transform=transform)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    if train:\n",
    "        transform_train = Compose([\n",
    "            RandomResizedCrop(img_size['height']),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=img_mean, std=img_std)\n",
    "        ])\n",
    "        # Get the dataloader for the training set\n",
    "        train_dataset = ImageFolder(TRAIN_DIR, transform=transform_train)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "        # Get the classification report for the training set\n",
    "        predictions, targets = get_pytorch_predictions_from_dataloader(model, train_dataloader)\n",
    "        print('Training set classification report:')\n",
    "        print(classification_report(targets, predictions.argmax(dim=1), target_names=classes))\n",
    "\n",
    "    # Get the classification report for the validation set\n",
    "    predictions, targets = get_pytorch_predictions_from_dataloader(model, val_dataloader)\n",
    "    print('Validation set classification report:')\n",
    "    print(classification_report(targets, predictions.argmax(dim=1), target_names=classes))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIGHTNING_LOGS_DIR = './lightning_logs'\n",
    "def load_latest_checkpoint(model_class, logs_dir=LIGHTNING_LOGS_DIR):\n",
    "    \"\"\"\n",
    "    Loads the latest checkpoint from the lightning_logs directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    model_class: PyTorch Lightning model class\n",
    "        The model class to use for loading the checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model: PyTorch Lightning model\n",
    "        The model loaded from the latest checkpoint.\n",
    "    \"\"\"\n",
    "    version_dirs = glob.glob(os.path.join(logs_dir, 'version_*'))\n",
    "    latest_version_dir = max(version_dirs, key=os.path.getmtime)\n",
    "    ckpt_files = glob.glob(os.path.join(latest_version_dir, 'checkpoints', '*.ckpt'))\n",
    "    latest_ckpt_file = max(ckpt_files, key=os.path.getmtime)\n",
    "    \n",
    "    # Load the checkpoint into a new instance of the model class\n",
    "    model = model_class.load_from_checkpoint(latest_ckpt_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [01:12<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Bedroom       0.86      0.96      0.91       116\n",
      "        Coast       0.91      0.98      0.94       260\n",
      "       Forest       0.98      0.96      0.97       228\n",
      "      Highway       1.00      0.89      0.94       160\n",
      "   Industrial       0.97      0.91      0.94       211\n",
      "  Inside city       0.95      0.93      0.94       208\n",
      "      Kitchen       0.97      0.99      0.98       110\n",
      "  Living room       0.98      0.88      0.93       189\n",
      "     Mountain       0.90      0.99      0.94       274\n",
      "       Office       0.99      0.99      0.99       115\n",
      " Open country       0.96      0.85      0.90       310\n",
      "        Store       0.95      1.00      0.97       215\n",
      "       Street       0.89      0.98      0.94       192\n",
      "       Suburb       0.99      0.98      0.98       141\n",
      "Tall building       0.96      0.96      0.96       256\n",
      "\n",
      "     accuracy                           0.95      2985\n",
      "    macro avg       0.95      0.95      0.95      2985\n",
      " weighted avg       0.95      0.95      0.95      2985\n",
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 47/47 [00:36<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Bedroom       0.86      0.96      0.91       100\n",
      "        Coast       0.89      0.97      0.93       100\n",
      "       Forest       1.00      0.89      0.94       100\n",
      "      Highway       0.98      0.92      0.95       100\n",
      "   Industrial       0.96      0.81      0.88       100\n",
      "  Inside city       0.86      0.86      0.86       100\n",
      "      Kitchen       0.99      0.86      0.92       100\n",
      "  Living room       0.88      0.87      0.87       100\n",
      "     Mountain       0.92      1.00      0.96       100\n",
      "       Office       0.98      0.99      0.99       100\n",
      " Open country       0.88      0.81      0.84       100\n",
      "        Store       0.88      0.96      0.92       100\n",
      "       Street       0.88      0.98      0.93       100\n",
      "       Suburb       1.00      0.96      0.98       100\n",
      "Tall building       0.89      0.97      0.93       100\n",
      "\n",
      "     accuracy                           0.92      1500\n",
      "    macro avg       0.92      0.92      0.92      1500\n",
      " weighted avg       0.92      0.92      0.92      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model from the latest checkpoint\n",
    "best_model = load_latest_checkpoint(ViTLightningModule)\n",
    "# Get best model metrics\n",
    "get_vit_metrics(best_model, train=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del conjunto de entrenamiento, la matriz de confusi贸n indica que el modelo logr贸 una precisi贸n de alrededor del 95% en la clasificaci贸n de las im谩genes, lo que significa que la mayor铆a de las im谩genes fueron clasificadas correctamente. La mayor铆a de las clases tienen una precisi贸n y recall bastante altos, con pocos casos de falsos positivos o negativos. Sin embargo, la clase \"Inside city\" tuvo una precisi贸n un poco m谩s baja en comparaci贸n con las otras clases.\n",
    "\n",
    "En el conjunto de validaci贸n, el modelo obtuvo una precisi贸n del 92%, lo que significa que las im谩genes se clasificaron correctamente en la mayor铆a de los casos. En general, la mayor铆a de las clases tuvieron una precisi贸n y recall similares a los del conjunto de entrenamiento, pero algunas clases, como \"Inside city\" y \"Open country\", tuvieron una precisi贸n ligeramente m谩s baja. Se puede concluir que el modelo tuvo un excelente rendimiento en la clasificaci贸n de las im谩genes, aunque la precisi贸n y recall var铆en seg煤n la clase y el conjunto de datos.\n",
    "\n",
    "En definitiva, el modelo fine-tuneado de Vision Transformer obtiene un rendimiento excelente, el cual est谩 al nivel de los mejores modelos de CNN. Adem谩s, no se observa una diferencia significativa en t茅rminos de performance entre los conjuntos de entrenamiento y validaci贸n, lo que sugiere una gran capacidad de generalizaci贸n a nuevos datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo final en `vit_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the models directory\n",
    "torch.save(best_model.state_dict(), os.path.join(models_dir, \"vit_model.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, pese a no haber realizado ning煤n tuneo el Vision Transformer iguala al mejor modelo de CNN del Hackaton y muestra tambi茅n una gran capacidad de generalizaci贸n, con un rendimiento de 0.95 de accuracy en training y 0.92 en validation. Podemos deducir que este buen rendimiento del modelo Vision Transformer se debe a su capacidad para capturar las dependencias y las interacciones globales entre las caracter铆sticas. Mientras que los modelos CNN tradicionales se basan en operaciones convolucionales y de agrupaci贸n para extraer caracter铆sticas locales y aplanarlas en un vector, los transformers utilizan mecanismos de autoatenci贸n que permiten interacciones globales entre todas las caracter铆sticas. Esto permite que los transformers modelen relaciones complejas entre las caracter铆sticas e identifiquen dependencias a larga distancia, lo que los hace particularmente efectivos para tareas como la clasificaci贸n de im谩genes.\n",
    "\n",
    "Adem谩s, la arquitectura jer谩rquica del Vision Transformer tambi茅n puede contribuir a su 茅xito en la tarea de clasificaci贸n de estilo art铆stico. La arquitectura le permite procesar im谩genes a m煤ltiples niveles de granularidad, desde caracter铆sticas locales hasta la imagen completa. Esto permite que el modelo aprenda representaciones que son m谩s adecuadas para tareas como la clasificaci贸n de im谩genes, y podr铆a explicar su fuerte rendimiento en este proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "no-estruc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2818b84fe0d8ee9ed89d361455090ef436eee20ee147624d1f156870c67bd555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
